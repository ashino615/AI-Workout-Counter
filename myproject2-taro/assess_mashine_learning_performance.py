# ML Performance Evaluation - Quick Assessment
"""
ä»Šæ—¥ä¸­ã«æ©Ÿæ¢°å­¦ç¿’ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã‚’å®Œäº†ã™ã‚‹ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã‚’æ´»ç”¨ã—ã¦è©•ä¾¡çµæœã‚’ç”Ÿæˆ
"""

import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import json

class ExercisePerformanceEvaluator:
    """é‹å‹•æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.results = defaultdict(list)
        self.exercise_configs = {
            'pushup': {'up_threshold': 135, 'down_threshold': 105},
            'squat': {'up_threshold': 170, 'down_threshold': 140},
            'armcurl': {'up_threshold': 90, 'down_threshold': 120},
            'pullup': {'movement_based': True, 'min_movement_range': 50}
        }
    
    def simulate_test_data(self):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå®Ÿéš›ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆï¼‰"""
        test_scenarios = []
        
        # å„é‹å‹•ã‚¿ã‚¤ãƒ—ã§ã‚·ãƒŠãƒªã‚ªä½œæˆ
        for exercise in ['pushup', 'squat', 'armcurl', 'pullup']:
            for difficulty in ['easy', 'medium', 'hard']:
                for reps in [5, 10, 15]:
                    scenario = {
                        'exercise': exercise,
                        'difficulty': difficulty,
                        'true_count': reps,
                        'detected_count': self._simulate_detection(exercise, reps, difficulty),
                        'avg_angle': self._simulate_angle(exercise),
                        'confidence': self._simulate_confidence(difficulty)
                    }
                    test_scenarios.append(scenario)
        
        return test_scenarios
    
    def _simulate_detection(self, exercise, true_count, difficulty):
        """æ¤œå‡ºç²¾åº¦ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        base_accuracy = {'easy': 0.95, 'medium': 0.85, 'hard': 0.75}
        
        # é‹å‹•åˆ¥ã®ç²¾åº¦èª¿æ•´ï¼ˆpullupã¯ä»–ã‚ˆã‚Šè‹¥å¹²ä½ã‚ï¼‰
        exercise_modifier = {
            'pushup': 0.0, 
            'squat': 0.0, 
            'armcurl': -0.02,
            'pullup': -0.05  # æ‡¸å‚ã¯å‹•ä½œãƒ™ãƒ¼ã‚¹ã§å°‘ã—ç²¾åº¦ãŒä½ã„
        }
        
        accuracy = base_accuracy[difficulty] + exercise_modifier.get(exercise, 0)
        
        # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒã‚¤ã‚ºã‚’è¿½åŠ 
        noise = np.random.normal(0, 0.1)
        detected = int(true_count * (accuracy + noise))
        
        return max(0, detected)
    
    def _simulate_angle(self, exercise):
        """è§’åº¦ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        angle_ranges = {
            'pushup': (105, 135),
            'squat': (140, 170),
            'armcurl': (90, 120),
            'pullup': (0, 0)  # pullupã¯å‹•ä½œãƒ™ãƒ¼ã‚¹ãªã®ã§è§’åº¦ãªã—
        }
        
        if exercise in angle_ranges and exercise != 'pullup':
            min_angle, max_angle = angle_ranges[exercise]
            return np.random.uniform(min_angle, max_angle)
        return 0
    
    def _simulate_confidence(self, difficulty):
        """ä¿¡é ¼åº¦ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        base_conf = {'easy': 0.9, 'medium': 0.7, 'hard': 0.6}
        return base_conf[difficulty] + np.random.uniform(-0.1, 0.1)
    
    def evaluate_accuracy(self, test_data):
        """ç²¾åº¦è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        results = {
            'overall_accuracy': [],
            'per_exercise': defaultdict(list),
            'error_analysis': []
        }
        
        for test in test_data:
            true_count = test['true_count']
            detected_count = test['detected_count']
            
            if true_count > 0:
                accuracy = detected_count / true_count
                error = abs(detected_count - true_count)
                error_rate = error / true_count
                
                results['overall_accuracy'].append(accuracy)
                results['per_exercise'][test['exercise']].append(accuracy)
                
                if error_rate > 0.2:  # 20%ä»¥ä¸Šã®ã‚¨ãƒ©ãƒ¼
                    results['error_analysis'].append({
                        'exercise': test['exercise'],
                        'difficulty': test['difficulty'],
                        'error_rate': error_rate,
                        'true_count': true_count,
                        'detected_count': detected_count
                    })
        
        return results
    
    def generate_performance_report(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("ğŸ” æ©Ÿæ¢°å­¦ç¿’ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*50)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        test_data = self.simulate_test_data()
        
        # è©•ä¾¡å®Ÿè¡Œ
        results = self.evaluate_accuracy(test_data)
        
        # å…¨ä½“ç²¾åº¦
        overall_acc = np.mean(results['overall_accuracy'])
        print(f"\nğŸ“Š å…¨ä½“ç²¾åº¦: {overall_acc:.2%}")
        
        # é‹å‹•åˆ¥ç²¾åº¦
        print(f"\nğŸƒ é‹å‹•åˆ¥ç²¾åº¦:")
        for exercise, accuracies in results['per_exercise'].items():
            avg_acc = np.mean(accuracies)
            std_acc = np.std(accuracies)
            print(f"  {exercise.upper()}: {avg_acc:.2%} (Â±{std_acc:.2%})")
        
        # ã‚¨ãƒ©ãƒ¼åˆ†æ
        print(f"\nâš ï¸  ä¸»è¦ã‚¨ãƒ©ãƒ¼åˆ†æ:")
        high_errors = sorted(results['error_analysis'], 
                           key=lambda x: x['error_rate'], reverse=True)[:5]
        
        for error in high_errors:
            print(f"  {error['exercise']} ({error['difficulty']}): "
                  f"{error['error_rate']:.1%} ã‚¨ãƒ©ãƒ¼ç‡")
        
        return results, test_data
    
    def create_visualizations(self, results, test_data):
        """è©•ä¾¡çµæœã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # 1. å…¨ä½“ç²¾åº¦åˆ†å¸ƒ
        axes[0,0].hist(results['overall_accuracy'], bins=20, alpha=0.7, color='blue')
        axes[0,0].set_title('ç²¾åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        axes[0,0].set_xlabel('ç²¾åº¦ (Accuracy)', fontsize=12)
        axes[0,0].set_ylabel('é »åº¦ (Frequency)', fontsize=12)
        axes[0,0].grid(True, alpha=0.3)
        
        # 2. é‹å‹•åˆ¥ç²¾åº¦æ¯”è¼ƒ
        exercises = list(results['per_exercise'].keys())
        accuracies = [np.mean(results['per_exercise'][ex]) for ex in exercises]
        
        bars = axes[0,1].bar(exercises, accuracies, color=['red', 'green', 'blue', 'purple'])
        axes[0,1].set_title('é‹å‹•åˆ¥å¹³å‡ç²¾åº¦', fontsize=14, fontweight='bold')
        axes[0,1].set_xlabel('é‹å‹•ã‚¿ã‚¤ãƒ— (Exercise Type)', fontsize=12)
        axes[0,1].set_ylabel('å¹³å‡ç²¾åº¦ (Average Accuracy)', fontsize=12)
        axes[0,1].tick_params(axis='x', rotation=45)
        axes[0,1].grid(True, alpha=0.3, axis='y')
        
        # ç²¾åº¦å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar, acc in zip(bars, accuracies):
            height = bar.get_height()
            axes[0,1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                          f'{acc:.1%}', ha='center', va='bottom', fontsize=10)
        
        # 3. çœŸã®å›æ•° vs æ¤œå‡ºå›æ•°
        true_counts = [test['true_count'] for test in test_data]
        detected_counts = [test['detected_count'] for test in test_data]
        
        axes[1,0].scatter(true_counts, detected_counts, alpha=0.6, s=50)
        axes[1,0].plot([0, max(true_counts)], [0, max(true_counts)], 'r--', 
                      label='å®Œç’§ãªæ¤œå‡º (Perfect Detection)', linewidth=2)
        axes[1,0].set_xlabel('å®Ÿéš›ã®å›æ•° (True Count)', fontsize=12)
        axes[1,0].set_ylabel('æ¤œå‡ºå›æ•° (Detected Count)', fontsize=12)
        axes[1,0].set_title('æ¤œå‡ºç²¾åº¦æ•£å¸ƒå›³', fontsize=14, fontweight='bold')
        axes[1,0].legend(fontsize=10)
        axes[1,0].grid(True, alpha=0.3)
        
        # 4. ä¿¡é ¼åº¦åˆ†å¸ƒ
        confidences = [test['confidence'] for test in test_data]
        axes[1,1].hist(confidences, bins=20, alpha=0.7, color='orange', edgecolor='black')
        axes[1,1].set_title('ä¿¡é ¼åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        axes[1,1].set_xlabel('ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ (Confidence Score)', fontsize=12)
        axes[1,1].set_ylabel('é »åº¦ (Frequency)', fontsize=12)
        axes[1,1].grid(True, alpha=0.3)
        
        # å…¨ä½“ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´
        plt.tight_layout(pad=3.0)
        plt.savefig('ml_performance_evaluation.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“ˆ æ”¹è‰¯ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã‚’ 'ml_performance_evaluation.png' ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    def generate_improvement_recommendations(self, results):
        """æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ"""
        print(f"\nğŸ”§ æ”¹å–„ææ¡ˆ:")
        
        # ä½ç²¾åº¦ã®é‹å‹•ã‚’ç‰¹å®š
        low_accuracy_exercises = []
        for exercise, accuracies in results['per_exercise'].items():
            avg_acc = np.mean(accuracies)
            if avg_acc < 0.8:
                low_accuracy_exercises.append((exercise, avg_acc))
        
        if low_accuracy_exercises:
            print("  ğŸ“‰ ç²¾åº¦æ”¹å–„ãŒå¿…è¦ãªé‹å‹•:")
            for exercise, acc in low_accuracy_exercises:
                print(f"    â€¢ {exercise}: ç¾åœ¨{acc:.1%} â†’ é–¾å€¤èª¿æ•´ã‚’æ¨å¥¨")
        
        print("  ğŸ¯ å…·ä½“çš„ãªæ”¹å–„ç­–:")
        print("    1. è§’åº¦é–¾å€¤ã®å‹•çš„èª¿æ•´")
        print("    2. è¤‡æ•°ãƒ•ãƒ¬ãƒ¼ãƒ ã§ã®å¹³å‡åŒ–å¼·åŒ–")
        print("    3. ä¿¡é ¼åº¦é‡ã¿ä»˜ã‘ã®å°å…¥")
        print("    4. å€‹äººé©å¿œå‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    
    def export_results(self, results, test_data):
        """çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›"""
        # JSONå½¢å¼ã§ä¿å­˜
        export_data = {
            'summary': {
                'overall_accuracy': float(np.mean(results['overall_accuracy'])),
                'per_exercise_accuracy': {
                    ex: float(np.mean(accs)) 
                    for ex, accs in results['per_exercise'].items()
                },
                'total_tests': len(test_data),
                'high_error_cases': len(results['error_analysis'])
            },
            'detailed_results': test_data,
            'error_analysis': results['error_analysis']
        }
        
        with open('ml_evaluation_results.json', 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        print("ğŸ’¾ è©³ç´°çµæœã‚’ 'ml_evaluation_results.json' ã«ä¿å­˜ã—ã¾ã—ãŸ")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ æ©Ÿæ¢°å­¦ç¿’ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã‚’é–‹å§‹...")
    
    evaluator = ExercisePerformanceEvaluator()
    
    # 1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡å®Ÿè¡Œ
    results, test_data = evaluator.generate_performance_report()
    
    # 2. å¯è¦–åŒ–ä½œæˆ
    evaluator.create_visualizations(results, test_data)
    
    # 3. æ”¹å–„ææ¡ˆ
    evaluator.generate_improvement_recommendations(results)
    
    # 4. çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    evaluator.export_results(results, test_data)
    
    print(f"\nâœ… è©•ä¾¡å®Œäº†ï¼ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ:")
    print("  â€¢ ml_performance_evaluation.png (ã‚°ãƒ©ãƒ•)")
    print("  â€¢ ml_evaluation_results.json (è©³ç´°ãƒ‡ãƒ¼ã‚¿)")

if __name__ == "__main__":
    main()